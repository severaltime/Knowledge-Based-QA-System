{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\"\"\"\n",
    "cwd_path: 当前工作路径\n",
    "knowledge_path: 知识库读取路径\n",
    "train_path: 训练数据读取路径\n",
    "test_path: 测试数据读取路径\n",
    "stopwords_path: 停用词读取路径\n",
    "embedding_path: 中文词向量读取路径\n",
    "\"\"\"\n",
    "\n",
    "cwd_path = os.getcwd()\n",
    "knowledge_path = os.path.join(cwd_path, 'QA_system_knowledge.txt')\n",
    "train_path = os.path.join(cwd_path, 'QA_system_train.txt')\n",
    "test_path = os.path.join(cwd_path, 'QA_system_test.txt')\n",
    "stopwords_path = os.path.join(cwd_path, 'QA_system_stop_words.txt')\n",
    "embedding_path = '/Users/xujie/Desktop/Python/nlp/data/word_embeding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数load_stopwords(), 读取停用词\n",
    "def load_stopwords(filename):\n",
    "    \"\"\"\n",
    "    IN: \n",
    "       filename: 停用词文件保存路径\n",
    "    OUT:\n",
    "       stopwords: 加载好的停用词数据\n",
    "    \"\"\"\n",
    "    # 读取停用词, 并输出\n",
    "    stopwords = set()\n",
    "    for line in open(filename):\n",
    "        stopwords.add(line.strip())\n",
    "    return stopwords\n",
    "\n",
    "stopwords = load_stopwords(stopwords_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import jieba.posseg as psg\n",
    "    \n",
    "# 基于jieba库, 定义分词函数tokenizer()\n",
    "def tokenizer(filename, stopwords):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "       filename:  目标文本文件保存路径\n",
    "       stopwords: 加载好的停用词数据\n",
    "    OUT:\n",
    "       texts: 去除停用词和低频词后的分词文本\n",
    "    \"\"\"\n",
    "    # 读取文件、分词、去除停用词\n",
    "    texts = []\n",
    "    for line in open(filename):\n",
    "        texts.append([token for token, _ in psg.cut(line.strip()) if token not in stopwords])\n",
    "    \n",
    "    # 统计单词词频\n",
    "    frequency = {}\n",
    "    for line in texts:\n",
    "        for word in line:\n",
    "            frequency[word] = frequency[word]+1 if word in frequency else 1\n",
    "    \n",
    "    # 去除词频等于1的单词\n",
    "    texts = [[word for word in line if frequency[word]>1] for line in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim import corpora\n",
    "\n",
    "# 基于gensim库, 生成词典及知识库的词袋数据(CountVectorize)\n",
    "def gen_corpora_and_dictionary(knowledge_path, train_path, filename, stopwords, loading=False):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "       knowledge_path: 知识库文件保存路径\n",
    "       train_path: 训练集文件保存路径\n",
    "       filename: 数据文本的保存加载路径\n",
    "       stopwords: 加载好的停用词数据\n",
    "       loading: bool型, 为True时直接载入数据\n",
    "    OUT:\n",
    "       dictionary: 知识库+训练集语料构成的词典\n",
    "       corpus: doc2bow后的知识库文本数据\n",
    "    \"\"\"\n",
    "    # 判断载入数据还是重新生成数据\n",
    "    if loading:\n",
    "        # 读取dictionary数据文件\n",
    "        with open(os.path.join(filename, 'dictionary'), 'rb') as f:\n",
    "            dictionary = pickle.load(f)\n",
    "        \n",
    "        # 读取corpus数据文件\n",
    "        with open(os.path.join(filename, 'corpus'), 'rb') as f:\n",
    "            corpus = pickle.load(f)\n",
    "    else:\n",
    "        # 读取知识库文本, 分词、去除停用词和低频词\n",
    "        knowledge_texts = tokenizer(knowledge_path, stopwords)\n",
    "    \n",
    "        # 读取训练集文本, 分词、去除停用词和低频词\n",
    "        train_texts = tokenizer(train_path, stopwords)\n",
    "    \n",
    "        # 基于gensim库中的corpora生成词典\n",
    "        dictionary = corpora.Dictionary(knowledge_texts+train_texts)\n",
    "    \n",
    "        # 将知识库文本转化为CountVectorize形式\n",
    "        corpus = [dictionary.doc2bow(text) for text in knowledge_texts]\n",
    "        \n",
    "        # 保存生成的dictionary数据到相应的文件\n",
    "        with open(os.path.join(filename, 'dictionary'), 'wb') as f:\n",
    "            pickle.dump(dictionary, f)\n",
    "                     \n",
    "        # 保存生成的corpus数据到相应的文件\n",
    "        with open(os.path.join(filename, 'corpus'), 'wb') as f:\n",
    "            pickle.dump(corpus, f)\n",
    "    return dictionary, corpus\n",
    "\n",
    "dictionary, corpus = gen_corpora_and_dictionary(knowledge_path, train_path, cwd_path, stopwords, loading=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models, similarities\n",
    "\n",
    "# 定义函数topk_similarity_idx()生成top_k相似度矩阵\n",
    "def topk_similarity_idx(filename, stopwords, k, loading=False):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "       filename: 需要计算top_k相似度的目标文件\n",
    "       stopwords: 加载好的停用词数据\n",
    "       k: 参数, 从知识库中抽取最相似背景数的大小\n",
    "       loading: bool型, 为True时直接载入数据\n",
    "    OUT:\n",
    "       sim_idx: 在知识库中提取与目标文件背景/问题最相关的k条知识所生成的矩阵\n",
    "    \"\"\"\n",
    "    # 生成输出文件sim_idx的保存路径\n",
    "    sim_path = os.getcwd()+'/sim_'+filename.split('/')[-1].split('_')[-1].split('.')[0]\n",
    "    \n",
    "    # 判断载入数据还是重新生成数据\n",
    "    if loading:\n",
    "        # 读取保存好的sim_idx数据文件\n",
    "        with open(sim_path, 'rb') as f:\n",
    "            sim_idx = pickle.load(f)\n",
    "    else:\n",
    "        # 读取dictionary数据文件\n",
    "        with open(os.path.join(os.getcwd(), 'dictionary'), 'rb') as f:\n",
    "            dictionary = pickle.load(f)\n",
    "        \n",
    "        # 读取corpus数据文件\n",
    "        with open(os.path.join(os.getcwd(), 'corpus'), 'rb') as f:\n",
    "            corpus = pickle.load(f)\n",
    "        \n",
    "        # 初始化潜在语意模型(Latent Semantic Indexing Model), 设置的主题数为10个\n",
    "        lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=10)\n",
    "        \n",
    "        # 将corpus数据转换到潜在语意空间(Latent Semantic Space), 并做Indexing操作\n",
    "        idx = similarities.MatrixSimilarity(lsi[corpus])\n",
    "        \n",
    "        # 在知识库中提取与目标文件背景/问题最相关的k条知识\n",
    "        sim_idx, tmp = [], []\n",
    "        for i, line in enumerate(open(filename)):\n",
    "            if i%6 == 0: # 提取背景(B)\n",
    "                tmp.extend([token for token, _ in psg.cut(line.strip()) if token not in stopwords])\n",
    "            if i%6 == 1: # 提取问题(Q)\n",
    "                tmp.extend([token for token, _ in psg.cut(line.strip()) if token not in stopwords])\n",
    "                sim_query = idx[lsi[dictionary.doc2bow(tmp)]]\n",
    "                sim_topk = [i for i, j in sorted(enumerate(sim_query), key=lambda item: -item[1])[:k]]\n",
    "                sim_idx.append(sim_topk)\n",
    "                tmp.clear()\n",
    "        \n",
    "        # 保存生成好的sim_idx数据文件\n",
    "        with open(sim_path, 'wb') as f:\n",
    "            pickle.dump(sim_idx, f)\n",
    "    return sim_idx\n",
    "\n",
    "sim_train = topk_similarity_idx(train_path, stopwords, 5, loading=True) # SIZE(10570, 5)\n",
    "sim_test = topk_similarity_idx(test_path, stopwords, 5, loading=True) # SIZE(2039, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义函数gen_and_save_embedding(), 提取原始(未处理)的词向量文件中的数据\n",
    "def gen_and_save_embedding(filename):\n",
    "    \"\"\"\n",
    "    IN: \n",
    "       filename: 原始(未处理)词向量文件的保存路径\n",
    "    OUT:\n",
    "       None\n",
    "    \"\"\"\n",
    "    # 初始化参数和变量\n",
    "    word2idx_ch, weights_ch = {}, []\n",
    "    \n",
    "    # 生成word2idx_ch(dict类型)和weights_ch(np.array类型)\n",
    "    for line in open(os.path.join(filename, 'ch_50d_word2vec')):\n",
    "        word = line.split()[0]\n",
    "        vector = list(map(float, line.split()[1:]))\n",
    "        \n",
    "        # 确保每个单词读入的词向量维度都是50，如果不是跳过该单词及其词向量\n",
    "        try:\n",
    "            assert len(vector) == 50\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        word2idx_ch[word] = len(word2idx_ch)\n",
    "        weights_ch.append(vector)\n",
    "    weights_ch = np.array(weights_ch, dtype=np.float64)\n",
    "    \n",
    "    # 保存生成的word2idx_ch数据到相应的文件\n",
    "    with open(os.path.join(filename, 'word2idx_ch'), 'wb') as f:\n",
    "        pickle.dump(word2idx_ch, f)\n",
    "                     \n",
    "    # 保存生成的weights_ch数据到相应的文件\n",
    "    with open(os.path.join(filename, 'weights_ch'), 'wb') as f:\n",
    "        pickle.dump(weights_ch, f)\n",
    "    return\n",
    "                     \n",
    "# gen_and_save_embedding(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数load_embedding(), 用于读取词向量相关文件\n",
    "def load_embedding(filename):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "       filename: 词向量文件的保存路径\n",
    "    OUT:\n",
    "       word2idx_ch: 词典类型, 单词和ID的对应表\n",
    "       weights_ch: Array类型, 词库的词向量矩阵, SIZE(598453, 50)\n",
    "    \"\"\"\n",
    "    # 读取word2idx_ch文件\n",
    "    with open(os.path.join(filename, 'word2idx_ch'), 'rb') as f:\n",
    "        word2idx_ch = pickle.load(f)\n",
    "        \n",
    "    # 读取weights_ch文件\n",
    "    with open(os.path.join(filename, 'weights_ch'), 'rb') as f:\n",
    "        weights_ch = pickle.load(f)\n",
    "    return word2idx_ch, weights_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 定义函数expend_token(), 在word2idx_ch\\weights原数据基础上扩充特殊标识符 \n",
    "def expend_token(embedding_path, expend_list=['</pad>', '</unk>', '</num>']):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "       texts: 带转换的目标文本(已由分词函数tokenizer()预处理)\n",
    "       embedding_path: 用于读取已保存的word2idx和weights文件\n",
    "       expend_list:  用于在word2idx中扩展特殊的标识符\n",
    "    OUT:\n",
    "       word2idx_ch: 增加特殊符号标记的word2idx文件(用于将单词转化到对应数字ID的map)\n",
    "       weights_ch: 增加特殊符号扩充的weights文件(np.array类型保存单词对应的词向量)\n",
    "    \"\"\"\n",
    "    # 从embedding_path中读取已保存的word2idx和weights文件\n",
    "    word2idx_ch, weights_ch = load_embedding(embedding_path)\n",
    "    \n",
    "    # expend_len为需要扩充的特殊标识符的数量\n",
    "    expend_len = len(expend_list)\n",
    "    \n",
    "    # 对word2idx_ch进行扩充\n",
    "    word2idx_ch = {k:(v+expend_len) for k, v in word2idx_ch.items()}\n",
    "    for i, token in enumerate(expend_list):\n",
    "        word2idx_ch[token] = i\n",
    "        \n",
    "    # 对weights_ch进行扩充, 新增特殊标识符的词向量统一为零向量\n",
    "    weights_ch = np.vstack((np.zeros((expend_len, 50), dtype=np.float64), weights_ch))\n",
    "    return word2idx_ch, weights_ch\n",
    "\n",
    "word2idx_ch, weights_ch = expend_token(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数word_list2idx(), 将目标文本word_list逐词转化为数字ID\n",
    "def word_list2idx(word_list, word2idx_ch):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "       list2idx: 将目标文本中的所有单词替换为对应的数字ID\n",
    "       word2idx_ch: 增加特殊符号标记的word2idx文件(用于将单词转化到对应数字ID的map)\n",
    "    OUT:\n",
    "       list2idx: 将目标文本中的所有单词替换为对应的数字ID\n",
    "    \"\"\"\n",
    "    # 将word_list中的单词逐一转化为对应的数字ID\n",
    "    list2idx = []\n",
    "    for word in word_list:\n",
    "        if word in word2idx_ch:\n",
    "            idx = word2idx_ch[word]\n",
    "        else:\n",
    "            # 若不存在于word2idx_ch中，按照是否出现数字划分为</num>和</unk>\n",
    "            idx = word2idx_ch['</num>'] if re.match(r'\\d+', word) else word2idx_ch['</unk>']\n",
    "        list2idx.append(idx)\n",
    "    return list2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_answer_split(knowledge_path, filename, stopwords, word2idx_ch, sim_idx):\n",
    "    \"\"\"\n",
    "    IN:\n",
    "       knowledge_path: 知识库文件保存路径\n",
    "       filename: 待加载的训练集或测试集文件路径\n",
    "       stopwords: 加载好的停用词数据\n",
    "       word2id_ch: 增加特殊符号标记的word2idx文件(用于将单词转化到对应数字ID的map)\n",
    "       weights_ch: 增加特殊符号扩充的weights文件(np.array类型保存单词对应的词向量)\n",
    "       sim_idx: 在知识库中提取与目标文件背景/问题最相关的k条知识所生成的矩阵\n",
    "    OUT:\n",
    "       quries_num: 目标文件中的Question/Answer pairs的数量\n",
    "       queries: 已转化为数字ID的题目+背景信息矩阵 SIZE(40760,)\n",
    "       answers: 已转化为数字ID的正确答案矩阵 SIZE(40760,)\n",
    "       labels: 记录正确答案和错误答案的位置信息 SIZE(40760,)\n",
    "    \"\"\"\n",
    "    # 读取、预处理知识库文本和目标文本\n",
    "    knowledge_texts = tokenizer(filename, stopwords)\n",
    "    texts = tokenizer(filename, stopwords)\n",
    "    \n",
    "    # 初始化相关参数和数据\n",
    "    queries_num = 0\n",
    "    tmp = []\n",
    "    queries, answers, labels = [], [], []\n",
    "    \n",
    "    # 生成queries, answers, labels\n",
    "    for i, line in enumerate(open(filename)):\n",
    "        if i%6 == 0:\n",
    "            queries_num += 1\n",
    "            # 在题目+背景信息中合并与其最相似的k条知识\n",
    "            for j in sim_idx[i//6]:\n",
    "                tmp.extend(knowledge_texts[j])\n",
    "            tmp.extend(texts[i])\n",
    "        elif i%6 == 1:\n",
    "            tmp.extend(texts[i])\n",
    "            queries.append(word_list2idx(tmp, word2idx_ch))\n",
    "        else:\n",
    "            \"\"\"\n",
    "               由于一道完整的题目由背景(B)、问题(Q)、一个正确答案(R)和三个错误答案(W1, W2, W3)组成\n",
    "               因此一道问题生成的样本为: query = (B+Q), answer = (R, W1, W2, W3), label(正确答案所做四个选项中的位置)\n",
    "            \"\"\"\n",
    "            # 统计、更新正确答案标签\n",
    "            if line[0] == 'R':\n",
    "                labels.append(i%4)\n",
    "                \n",
    "            answers.append(word_list2idx(texts[i], word2idx_ch))\n",
    "                \n",
    "            # 清空、初始化tmp\n",
    "            if i%6 == 5:\n",
    "                tmp.clear()\n",
    "    return queries_num, queries, answers, labels\n",
    "\n",
    "train_query_num, train_query, train_answer, train_label = query_answer_split(knowledge_path, \n",
    "                                                           train_path, stopwords, word2idx_ch, sim_train)\n",
    "test_query_num, test_query, test_answer, test_label = query_answer_split(knowledge_path, \n",
    "                                                           test_path, stopwords, word2idx_ch, sim_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入tensorflow, keras深度学习框架\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import optimizers, losses\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置问题的最大长度和答案的最大长度(超出最大长度的部分截断,未达到最大长度时用Padding进行填充)\n",
    "max_query_len = 100 # max([len(line) for line in queries])\n",
    "max_ans_len = 30 # max(max([len(line) for line in true_answers]), max([len(line) for line in false_answers]))\n",
    "\n",
    "# 对训练集的问题和答案进行处理,统一长度\n",
    "train_query = pad_sequences(np.array(train_query), value = 0, padding = 'post', maxlen=max_query_len) # SIZE(, 100)\n",
    "train_answer = pad_sequences(np.array(train_answer), value = 0, padding = 'post', maxlen=max_ans_len)\n",
    "train_answer = train_answer.reshape([train_query_num, 4, max_ans_len]) # SIZE(, 4, 30)\n",
    "\n",
    "# 对测试集的问题和答案进行处理,统一长度\n",
    "test_query = pad_sequences(np.array(test_query), value = 0, padding = 'post', maxlen=max_query_len) # SIZE(, 100)\n",
    "test_answer = pad_sequences(np.array(test_answer), value = 0, padding = 'post', maxlen=max_ans_len)\n",
    "test_answer = test_answer.reshape([test_query_num, 4, max_ans_len]) # SIZE(, 4, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继承keras中的Model模块,DIY深度神经网络\n",
    "class Model(keras.Model):\n",
    "    def __init__(self, weights=weights_ch, units=64):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = Embedding(weights.shape[0], weights.shape[1], weights=[weights], trainable=False)\n",
    "        self.lstm1 = Bidirectional(LSTM(units//2, return_sequences=True))\n",
    "        self.lstm2 = Bidirectional(LSTM(units//2))\n",
    "        self.dropout = Dropout(0.4)\n",
    "        self.normalization = BatchNormalization()\n",
    "        self.dense = Dense(4, activation='softmax')\n",
    "    \n",
    "    # 定义biLSTM层,结构: 嵌入层+biLSTM1+随机失活层+biLSTM2+批标准化层\n",
    "    def biLSTM(self, tmp):\n",
    "        res = self.embedding(tmp)\n",
    "        res = self.lstm1(res)\n",
    "        res = self.dropout(res)\n",
    "        res = self.lstm2(res)\n",
    "        res = self.normalization(res)\n",
    "        return res\n",
    "    \n",
    "    # 定义问题和答案选项的相似度计算(余弦相似度)\n",
    "    def get_cosSim(self, q, a):\n",
    "        q_norm = tf.sqrt(tf.reduce_sum(q*q, 1))\n",
    "        a_norm = tf.sqrt(tf.reduce_sum(a*a, 1))\n",
    "        mul = tf.reduce_sum(q*a, 1)\n",
    "        # 考虑epsilon防止计算余弦相似度时分母除0\n",
    "        epsilon = tf.fill(mul.get_shape(), 1.0)\n",
    "        cosSim = tf.divide(mul, q_norm*a_norm+epsilon)\n",
    "        cosSim = tf.expand_dims(cosSim, axis=1)\n",
    "        return cosSim\n",
    "    \n",
    "    # 定义深度网络的前馈计算机制\n",
    "    def call(self, query, answer, training=None):\n",
    "        # 输出问题经过biLSTM层作用后的隐变量\n",
    "        query = self.biLSTM(query)\n",
    "        cosSim = []\n",
    "        # 针对answer中四个选项(A, B, C, D),计算选项与问题的余弦相似度\n",
    "        for option in tf.unstack(answer, axis=1):\n",
    "            # 输出答案选项经过biLSTM层作用后的隐变量\n",
    "            option = self.biLSTM(option)\n",
    "            cosSim.append(self.get_cosSim(query, option))\n",
    "        # 合并 SIZE(, 100)\n",
    "        merged = tf.concat(cosSim, axis=1)\n",
    "        # 加入一个全连接层,并选择激活函数为softmax函数\n",
    "        output = self.dense(merged)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 --- Batch ID: 1, Loss: 2.8219, Acc: 0.5938 ---\n",
      "Iteration 1 --- Batch ID: 11, Loss: 2.9576, Acc: 0.4375 ---\n",
      "Iteration 1 --- Batch ID: 21, Loss: 2.8838, Acc: 0.5469 ---\n",
      "Iteration 1 --- Batch ID: 31, Loss: 2.8925, Acc: 0.4688 ---\n",
      "Iteration 1 --- Batch ID: 41, Loss: 2.9231, Acc: 0.4531 ---\n",
      "Iteration 1 --- Batch ID: 51, Loss: 2.8609, Acc: 0.5156 ---\n",
      "Iteration 1 --- Batch ID: 61, Loss: 2.9883, Acc: 0.3281 ---\n",
      "Iteration 1 --- Batch ID: 71, Loss: 2.9201, Acc: 0.4688 ---\n",
      "Iteration 1 --- Batch ID: 81, Loss: 2.8574, Acc: 0.5625 ---\n",
      "Iteration 1 --- Batch ID: 91, Loss: 2.8998, Acc: 0.4844 ---\n",
      "Iteration 1 --- Batch ID: 101, Loss: 2.8942, Acc: 0.4688 ---\n",
      "Iteration 1 --- Batch ID: 111, Loss: 2.8608, Acc: 0.5469 ---\n",
      "Iteration 1 --- Batch ID: 121, Loss: 2.9181, Acc: 0.4844 ---\n",
      "Iteration 1 --- Batch ID: 131, Loss: 2.8820, Acc: 0.5000 ---\n",
      "Iteration 1 --- Batch ID: 141, Loss: 2.8384, Acc: 0.5625 ---\n",
      "Iteration 1 --- Batch ID: 151, Loss: 3.0166, Acc: 0.3281 ---\n",
      "--- Iteration ID: 1, Loss_Val: 3.1061, Acc_Val: 0.2344 ---\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "lr = 0.01 # 优化器学习率\n",
    "epochs = 1 # 迭代次数\n",
    "batch_size = 64 # 批量\n",
    "perm = [i for i in range(train_query_num)] # 随机抽样\n",
    "\n",
    "train_label = np.array(train_label)\n",
    "test_label = np.array(test_label)\n",
    "\n",
    "# model = Model()\n",
    "optimizer = optimizers.Adam(lr) # 优化器:Adam\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(perm)\n",
    "    for i in range(train_query_num//batch_size):\n",
    "        \n",
    "        # 将训练集随机打乱之后分批次进行训练\n",
    "        sample = perm[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # ouput为biLSTM深度网络的输出结果\n",
    "            output = model(train_query[sample], train_answer[sample]) \n",
    "            one_hot = tf.one_hot(train_label[sample], depth = 4)\n",
    "            # 计算交叉熵损失值\n",
    "            loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=one_hot, logits=output)\n",
    "            loss = tf.reduce_sum(loss)/batch_size\n",
    "            \n",
    "        # 每十个批次统计模型在训练集上的损失值和准确率\n",
    "        if i%10==0:\n",
    "            acc = tf.equal(tf.argmax(output, axis=1), tf.argmax(one_hot, axis=1))\n",
    "            acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "            print('Iteration {} --- Batch ID: {}, Loss: {:.4f}, Acc: {:.4f} ---'.format(epoch+1, i+1, loss, acc))\n",
    "            \n",
    "        # 根据损失和BP算法更新model中的训练参数\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "    # 每一轮迭代统计模型在验证集上的损失值和准确率\n",
    "    sample = np.random.randint(0, test_query_num, batch_size)\n",
    "    output = model(test_query[sample], test_answer[sample]) \n",
    "    one_hot = tf.one_hot(test_label[sample], depth = 4)\n",
    "    loss_val = tf.nn.sigmoid_cross_entropy_with_logits(labels=one_hot, logits=output)\n",
    "    loss_val = tf.reduce_sum(loss_val)/batch_size\n",
    "    acc_val = tf.equal(tf.argmax(output, axis=1), tf.argmax(one_hot, axis=1))\n",
    "    acc_val = tf.reduce_mean(tf.cast(acc_val, tf.float32))\n",
    "    print('--- Iteration ID: {}, Loss_Val: {:.4f}, Acc_Val: {:.4f} ---'.format(epoch+1, loss_val, acc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "weight_path = os.path.join(cwd_path, 'model_weight_v0.h5')\n",
    "model.save_weights(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
